# Hyperparameters and settings for the Simformer model and experiments

model:
  embedding_dim: 128
  num_layers: 6
  num_heads: 8
  d_ff: 512
  dropout: 0.1

training:
  learning_rate: 0.0001
  batch_size: 64
  num_epochs: 100
  num_simulations: 50000
  num_posterior_samples: 1000

tasks:
  gaussian:
    num_params: 10
    num_data: 10
  two_moons:
    # Note: M_E mask expects 10 data points, but simulator provides 2.
    # This will need to be handled, e.g., by padding or using a different mask.
    # For now, aligning with the mask.
    num_params: 2
    num_data: 10
  gaussian_mixture:
    num_params: 2
    num_data: 10 # Assuming same structure as two_moons
  slcp:
    num_params: 4
    num_data: 8
  tree:
    # Assuming 7 parameters and 3 data points based on M_E_tree structure
    num_params: 7
    num_data: 3
  hmm:
    num_params: 10
    num_data: 10
  lotka_volterra:
    # sbi simulator for LV uses 4 params and returns 2 summary stats
    num_params: 4
    num_data: 2
  hodgkin_huxley:
    # sbi simulator for HH uses 3 params and returns 8 summary stats
    num_params: 3
    num_data: 8

evaluation:
  c2st_folds: 5

environment:
  device: 'cuda' # 'cuda' or 'cpu'
